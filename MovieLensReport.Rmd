---
title: "MovieLens Report"
author: "Xiaoshuo Liu"
date: "April 8, 2019"
output:
  word_document: default
  pdf_document: default
---

I. Introduction

The MovieLens data set was provided by the course and was examined for a functional recommendation system. The goal of this project is to construct a recommendation model that provide the right (or closest) movies to a user based on the potential that a high rating will be given by the person. 

After testing through different models, I found that although k nearest neighbors and random forest methods would be used to create a model, their RMSEs were too high to be considered valuable (RMSE-knn >4, RMSE-rborist>1). Then, I used "recommenderlab" package to run recommendation-specific methods to construct models. The finding was that the "POPULAR" method generated the lowest RMSE. I also used principal component analysis to seek possibility of shrinking down the number of predictors or potentially improving the RMSE. I was able to reduce the number by five while keeping RMSE the same. I spent over two months working part time on this project and decided not to go further due to my time constraints. I thought it would be better to submit the project than to give up after numerous hours of work. To my fellow graders, I would love to hear your feedback so I could possibly improve the model further in the future when I have the time to work on it again.    

I would like to note that I used train sets to train the model, test sets to validate, and validation sets to do the final test. This confusion was created due to my initial impression with the "edx" and "validation" sets. I thought that "edx" would be used for train and test while "validation" was for the final validation. The validation set was only used once to test the final model. 

Another thing that I wanted to point out was that because I had to do this project on a laptop, the processing power was rather limited. The laptop was my only PC device to work on, and I had to use it for other things. Therefore, I had to shrink down the size of the sets so I could get results out for a reasonable amount of time. This could be a reason why my final model was not even good enough to reach the highest RMSE in the rubric. 

II. Preparing the Data

The main data sets were generated by the script provided on the course page. Set edx was for training and testing while Set validation was for validating the model. The course has already required us to explore the data set, so I skipped the exploratory analysis. Knowing that the date and time in the set were labeled in the timestamp format, I first transformed the timestamp format to normal date and time format, then splitted them into separate colunms for easier analysis later. 

First, I loaded the original data. 
```{r original-data}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
The original sets are quite large to handle. Therefore I decided to slice them into smallers sizes. The reason why a random draw was not used was because I wanted to give each user enough observations to better adapt the factorization method. 
```{r smallerSets}
edx <- edx[c(1:300000, 3000000:3300000, 6000000:6300000),]
validation <- validation[c(1:33333, 300000:333333, 600000:633333),]
```

Then, I separated the dates and time. 

```{r date-time}
#Load all the packages needed. 
library(tidyverse)
library(randomForest)
library(rpart)
library(caret)
library(purrr)
library(chron)
library(stringr)


#Transform timestamp into normal date/time format
edx <- edx %>% mutate(timestamp = as.POSIXct(edx$timestamp, origin = "1970-01-01", tz = "GMT"))

validation <- validation %>% mutate(timestamp = as.POSIXct(validation$timestamp, origin = "1970-01-01", tz = "GMT"))


#Splitting date and time
edx <- edx %>% separate(col = timestamp, into = c("date", "time"), sep = " ")

validation <- validation %>% separate(col = timestamp, into = c("date", "time"), sep = " ")

#Check the format of the two new columns
class(edx$date)
class(validation$date)

#Separate year, month, and date from the "date" column
datePrep <- edx$date
ratingYear <- str_sub(string = datePrep, start = 1, end = 4)
ratingMonth <- str_sub(string = datePrep, start = 6, end = 7)
ratingDay <- str_sub(string = datePrep, start = 9, end = 10)
edx <- cbind(edx, ratingYear)
edx <- cbind(edx, ratingMonth)
edx <- cbind(edx, ratingDay)

datePrep <- validation$date
ratingYear <- str_sub(string = datePrep, start = 1, end = 4)
ratingMonth <- str_sub(string = datePrep, start = 6, end = 7)
ratingDay <- str_sub(string = datePrep, start = 9, end = 10)
validation <- cbind(validation, ratingYear)
validation <- cbind(validation, ratingMonth)
validation <- cbind(validation, ratingDay)

#Remove the original "date" column to slim down the set. 
edx <- edx[, -4]
validation <- validation[, -4]

#Separate hour, minute, and second from the "time" column.
timePrep <- edx$time
ratingHour <- str_sub(string = timePrep, start = 1, end = 2)
ratingMin <- str_sub(string = timePrep, start = 4, end = 5)
ratingSec <- str_sub(string = timePrep, start = 7, end = 8)
edx <- cbind(edx, ratingHour)
edx <- cbind(edx, ratingMin)
edx <- cbind(edx, ratingSec)

timePrep <- validation$time
ratingHour <- str_sub(string = timePrep, start = 1, end = 2)
ratingMin <- str_sub(string = timePrep, start = 4, end = 5)
ratingSec <- str_sub(string = timePrep, start = 7, end = 8)
validation <- cbind(validation, ratingHour)
validation <- cbind(validation, ratingMin)
validation <- cbind(validation, ratingSec)

#Remove the original "time" column to slim down the set. 
edx <- edx[, -4]
validation <- validation[, -4]

#Remove all the values created during this process. 
rm(datePrep)
rm(ratingDay)
rm(ratingHour)
rm(ratingMin)
rm(ratingMonth)
rm(ratingSec)
rm(ratingYear)
rm(timePrep)
```

After getting the rating date and time prepared. I also extracted the release year out of the "title" column as a variable. 

```{r releaseYear}
#Create a column of release year. 
releasePrep <- edx$title
releaseYear <- str_sub(string = releasePrep, start = -5, end = -2)
edx <- cbind(edx, releaseYear)

releasePrep <- validation$title
releaseYear <- str_sub(string = releasePrep, start = -5, end = -2)
validation <- cbind(validation, releaseYear)

#Remove the values no longer used. 
rm(releasePrep)
rm(releaseYear)
```

I also converted the "genres" column into a series of columns that are dummy variables for later analysis. 

```{r edx genreSplit}
library(dplyr)
library(tidyr)

#Separate genres under the same title 
edx <- separate_rows(data = edx, genres, sep = "\\|", convert = FALSE)

#Transform character values into dummy variables
edx <- edx %>% mutate(Drama = ifelse(genres == "Drama", 1, 0)) %>%
  mutate(Crime = ifelse(genres == "Crime", 1, 0)) %>%
  mutate(Action = ifelse(genres == "Action", 1, 0)) %>%
  mutate(Adventure = ifelse(genres == "Adventure", 1, 0)) %>%
  mutate(Sci_Fi = ifelse(genres == "Sci-Fi", 1, 0)) %>%
  mutate(Thriller = ifelse(genres == "Thriller", 1, 0)) %>%
  mutate(Comedy = ifelse(genres == "Comedy", 1, 0)) %>%
  mutate(Mystery = ifelse(genres == "Mystery", 1, 0)) %>%
  mutate(Romance = ifelse(genres == "Romance", 1, 0)) %>%
  mutate(Animation = ifelse(genres == "Animation", 1, 0)) %>%
  mutate(Children = ifelse(genres == "Children", 1, 0)) %>%
  mutate(Fantasy = ifelse(genres == "Fantasy", 1, 0)) %>%
  mutate(War = ifelse(genres == "War", 1, 0)) %>%
  mutate(Horror = ifelse(genres == "Horror", 1, 0)) %>%
  mutate(Musical = ifelse(genres == "Musical", 1, 0)) %>%
  mutate(Western = ifelse(genres == "Western", 1, 0)) %>%
  mutate(Film_Noir = ifelse(genres == "Film-Noir", 1, 0)) %>%
  mutate(Documentary = ifelse(genres == "Documentary", 1, 0)) %>%
  mutate(IMAX = ifelse(genres == "IMAX", 1, 0)) %>%
  mutate(no_genres_listed = ifelse(genres == "(no genres listed)", 1, 0))

#Deleting the "genres" column after converting all genres into dummy variables and title column (assume movieId would just do the same). 
edx <- within(edx, rm(genres))
edx <- within(edx, rm(title))

```

After all the foundation work, partitions were created for modeling later. 
```{r partitions}
#Creating Partitions
rating <- edx$rating
set.seed(1)
testIndex <- createDataPartition(rating, times = 1, p = 0.5, list = FALSE)
trainSet <- edx[-testIndex,]
testSet <- edx[testIndex,]

#Making sure that the two sets share users and movies. 
testSet <- testSet %>% 
          semi_join(trainSet, by = "movieId") %>% 
          semi_join(trainSet, by = "userId")

#releasing memory
rm(edx)
rm(testIndex)
rm(rating)
```

Splitting genres for the validation set. It was done after removing edx because of RAM limit. 
```{r validation genres split}
#Applying the same method to the validation set
validation <- separate_rows(data = validation, genres, sep = "\\|", convert = FALSE)

validation <- validation %>% mutate(Drama = ifelse(genres == "Drama", 1, 0)) %>%
  mutate(Crime = ifelse(genres == "Crime", 1, 0)) %>%
  mutate(Action = ifelse(genres == "Action", 1, 0)) %>%
  mutate(Adventure = ifelse(genres == "Adventure", 1, 0)) %>%
  mutate(Sci_Fi = ifelse(genres == "Sci-Fi", 1, 0)) %>%
  mutate(Thriller = ifelse(genres == "Thriller", 1, 0)) %>%
  mutate(Comedy = ifelse(genres == "Comedy", 1, 0)) %>%
  mutate(Mystery = ifelse(genres == "Mystery", 1, 0)) %>%
  mutate(Romance = ifelse(genres == "Romance", 1, 0)) %>%
  mutate(Animation = ifelse(genres == "Animation", 1, 0)) %>%
  mutate(Children = ifelse(genres == "Children", 1, 0)) %>%
  mutate(Fantasy = ifelse(genres == "Fantasy", 1, 0)) %>%
  mutate(War = ifelse(genres == "War", 1, 0)) %>%
  mutate(Horror = ifelse(genres == "Horror", 1, 0)) %>%
  mutate(Musical = ifelse(genres == "Musical", 1, 0)) %>%
  mutate(Western = ifelse(genres == "Western", 1, 0)) %>%
  mutate(Film_Noir = ifelse(genres == "Film-Noir", 1, 0)) %>%
  mutate(Documentary = ifelse(genres == "Documentary", 1, 0)) %>%
  mutate(IMAX = ifelse(genres == "IMAX", 1, 0)) %>%
  mutate(no_genres_listed = ifelse(genres == "(no genres listed)", 1, 0))


validation <- within(validation, rm(genres))
validation <- within(validation, rm(title))
```


III. Predictive Modeling

Although in the lectures, Dr. Irizarry used matrix factorization to construct his sample model, I want to try the two predictive models, K nearest neighbors and regression tree. I noticed that the train and test sets were still quite big for testing models, so I created a smaller set. The "movieID" column was also removed because it caused RAM error on my computer which made it unable to complete model training and testing. 

```{r Tiny}
#Create smaller dataset for model testing
set.seed(1)
tinyTrain <- trainSet[c(1:33333, 300000:333333, 600000:633334),]
tinyTest <- testSet[c(1:33333, 300000:333333, 600000:633334),]

#Remove the original train and test sets. 
rm(trainSet)
rm(testSet)

#Eliminating "movieID" only for predictive modeling (causing RAM issue) 
trainMovieId <- tinyTrain$movieId
testMovieId <- tinyTest$movieId
tinyTrain <- within(tinyTrain, rm(movieId))
tinyTest <- within(tinyTest, rm(movieId))
```

1. K Nearest Neighbors
To reduce lengthy processing time, I adopted the control from the lectures. 
```{r knn}
#Create dataframe for predictors. 
predictors <- within(tinyTrain, rm(rating))
#k nearest neighbors
control <- trainControl(method = "cv", number = 3, p = .8)
pred <- tinyTrain[, 2]
train_knn <- train(predictors, pred,
                   method = "knn",
                   tuneGrid = data.frame(k = seq(15,25,2)),
                   trControl = control)
train_knn$bestTune
pred <- as.factor(pred)
fit_knn <- knn3(predictors, pred, k = train_knn$bestTune)
testPredictors <- within(tinyTest, rm(rating))
y_hat_knn <- predict(fit_knn,
                     testPredictors,
                     type = "class")
testPred <- tinyTest[,2]
cm_knn <- confusionMatrix(as.factor(y_hat_knn), as.factor(testPred))
rmse_knn <- RMSE(as.numeric(testPred), as.numeric(y_hat_knn))
cm_knn
rmse_knn
```
The result showed an accuracy of ~40% and a terrible RMSE of 4.16. Clearly, knn was not able to construct a high quality recommendation system. 


2. Classification Tree (Rborist w/ five-fold cross validation)
To reduce lengthy processing time, I adopted the control from the lectures. 
```{r rborist}
library(Rborist)
control <- trainControl(method = "cv", number = 3, p = 0.8)
grid <- expand.grid(minNode = c(2,5), predFixed = c(15,20,25))
pred <- as.factor(tinyTrain[, 2])

train_rf <- train(predictors,
                  pred,
                  method = "Rborist",
                  nTree = 50,
                  trControl = control,
                  tuneGrid = grid,
                  nSample = 5000)
train_rf$bestTune

fit_rf <- Rborist(predictors, pred, 
                  minNode = train_rf$bestTune$minNode,
                  predFixed = train_rf$bestTune$predFixed)

testPredictors <- within(tinyTest, rm(rating))
testPred <- tinyTest[,2]
pred_rf <- predict(fit_rf, testPredictors)
y_hat_rf <- as.factor(levels(pred)[predict(fit_rf, testPredictors)$yPred])
testPred <- as.factor(testPred)
cm_rf <- confusionMatrix(y_hat_rf, testPred)
rmse_rf <- RMSE(as.numeric(testPred), as.numeric(y_hat_rf))
cm_rf
rmse_rf
```
The result showed an accuracy of ~77%, which was a big improvement compared to the knn result. Also, the RMSE improved from 4.16 to 1.41. Based on the project's rubric, the RMSE was still not good enough. It wasn't worth testing on the validation set. 

In conclusion, both knn and regression tree methods failed to construct satisfying recommendation systems. Before moving into matrix factorization, I wanted to try principal component analysis and see if the procedure could help improve the results of knn and regression tree a bit. 

IV. Principal Component Analysis

There are over twenty columns which represent different genres in the set. This has caused some serious lags on processing time for modeling. I wanted to find out whether a PCA can shrink down the number of variables needed for the prediction. 

```{r PCA}
#Using Principal Component Analysis to shrink down the number of predictors

pca <- prcomp(tinyTrain[,c(10:29)], center = FALSE, scale. = FALSE)
pcaTest <- prcomp(tinyTest[,c(10:29)], center = FALSE, scale. = FALSE)

summary(pca)

#Here we can see that the first 15 components capture ~98% of the variability. Let's use these 15 which represent genres and the rest of regular predictors. 

axes <- predict(pca, newdata = tinyTrain)
axesTest <- predict(pcaTest, newdata = tinyTest)

pcaData <- cbind(tinyTrain, axes)
pcaTestData <- cbind(tinyTest, axesTest)
```

After getting the PCA variables, I tested them with the random forest model. The knn model's results were too bad to test. 
```{r PCArf}
#Create dataframe for predictors and predicted values. 
PCApredictors <- pcaData[,c(1,3:9,30:44)]
PCApred <- pcaData[,2]

PCAtestPredictors <- pcaTestData[,c(1,3:9,30:44)]
PCAtestPred <- pcaTestData[,2]

#Random Forest
PCAcontrol <- trainControl(method = "cv", number = 3, p = 0.8)
PCAgrid <- expand.grid(minNode = c(2,5), predFixed = c(10,15,20))
PCApred <- as.factor(tinyTrain[, 2])

PCAtrain_rf <- train(PCApredictors,
                  PCApred,
                  method = "Rborist",
                  nTree = 50,
                  trControl = PCAcontrol,
                  tuneGrid = PCAgrid,
                  nSample = 5000)
PCAtrain_rf$bestTune

PCAfit_rf <- Rborist(PCApredictors, PCApred, 
                  minNode = PCAtrain_rf$bestTune$minNode,
                  predFixed = PCAtrain_rf$bestTune$predFixed)

PCApred_rf <- predict(PCAfit_rf, PCAtestPredictors)
PCAy_hat_rf <- as.factor(levels(PCApred)[predict(PCAfit_rf, PCAtestPredictors)$yPred])
PCAtestPred <- as.factor(PCAtestPred)
PCAcm_rf <- confusionMatrix(PCAy_hat_rf, PCAtestPred)
PCArmse_rf <- RMSE(as.numeric(PCAtestPred), as.numeric(PCAy_hat_rf))
PCAcm_rf
PCArmse_rf
```
As the result entailed, the PCA variables didn't improve either the accuracy or the RMSE of the model. Instead, it even made the results worse. It was about time to use the matrix factorization method taught in the lectures. 

V. "Recommenderlab" Package

Dr. Irizarry mentioned the "recommenderlab" package at the end of his lecture on a recommendation system. After some research, I learned that the package was specifically written for recommendation system research. The methods available are very similar to the ones introduced by Dr. Irizarry in his lectures. Therefore, instead of replicating and improving the process used in the lecture, I decided to utilize the package for further model construction. For more detailed description of functions that were used, please view the package's document. 

The Recommenderlab package had a function called "evaluate", which evaluates the outcome of one or more methods with the same data set. The methods were user-based collaborative filtering, item-based collaborative filtering, singular value decomposition(SVD) with column-mean imputation, funk SVD, association rule-based recommender, popular items, randomly chosen items for comparison, and re-recommend liked items. Therefore, I decided to evaluate all the methods first before putting them into training. 

First, I added the a crucial component, movie IDs, back to the set to where it used to be. I took it out earlier because it was causing RAM issue when running knn and Rborist models. 

```{r movieIdBack}
#Using the package "recommendarlab" 
library(recommenderlab)
library(dplyr)
library(tibble)

#Add "movieId" back into the sets.
tinyTrain <- add_column(tinyTrain, movieId = trainMovieId, .after = 1)
tinyTest <- add_column(tinyTest, movieId = testMovieId, .after = 1)
```
Once the sets were restored back to their original format, I started using the "recommenderlab" package for modeling. 

```{r rmdr}
#Getting a recommender training set.
recomTrain <- tinyTrain

#Convert the dataframe to a rating matrix. 
recomTrain <- as(recomTrain, "realRatingMatrix")

#Create an evaluation scheme. 
e <- evaluationScheme(recomTrain, method = "cross-validation", train = 0.5, k = 3, given = -1)
e

#Evaluate different methods 
algos <- list(
  UBCF = list(name = "UBCF", param = NULL),
  IBCF = list(name = "IBCF", param = NULL),
  SVD = list(name = "SVD", param = NULL),
  SVDF = list(name = "SVDF", param = NULL),
  AR = list(name = "AR", param = NULL),
  POPULAR = list(name = "POPULAR", param = NULL),
  RANDOM = list(name = "RANDOM", param = NULL),
  RERECOMMEND = list(name = "RERECOMMEND", param = NULL)
)

evlist <- evaluate(e, algos, type = "ratings")

plot(evlist, legend = "topright")
evResults <- avg(evlist)
evResults
```

As we can see from the graph and the result sheet, the method "POPULAR" (based on popular items across users) had the lowest RMSE. I created a model with this method.
```{r POPULAR}
#Create POPULAR recommender
r <- Recommender(getData(e, "train"), "POPULAR")
r

#Create predictions
p <- recommenderlab::predict(r, getData(e, "known"), type = "ratings")
p

#Calculate accuray
Accu <- as.data.frame(calcPredictionAccuracy(p, getData(e, "unknown"), byUser = TRUE)) %>% na.omit() %>%
        colMeans()
Accu
```
As we can see from the result, the RMSE improved from the result from the evaluation process. However, it was still far from the desired RMSE range provided by the rubric. Therefore, I wanted to ensemble several methods to improve the results. 

The "recommenderlab" package offered a "Hybrid Recommender" function, which essnetially allowed combining several methods for one recommendation system. I chose the top three results, user-based collaborative filtering (UBCF), funk SVD (SVDF), and popular items (POPULAR) from the evaluation results and see if the RMSE would be improved. 

```{r Hybrid}
#Create hybrid recommender
rHy <- HybridRecommender(
  Recommender(getData(e, "train"), "UBCF"),
  Recommender(getData(e, "train"),"SVDF"),
  Recommender(getData(e, "train"),"POPULAR"),
  weights = NULL
)
rHy

#Create predictions
pHy <- recommenderlab::predict(rHy, getData(e, "known"), type = "ratings")
pHy

#Calculate accuray
AccuHy <- as.data.frame(calcPredictionAccuracy(pHy, getData(e, "unknown"), byUser = TRUE)) %>% na.omit() %>%colMeans()
AccuHy
```
As we could see from the results, the hybrid model improved the RMSE by a tiny margin. This was still far from our desired results, so I decided to engage in further method research.

VI. "Recommenderlab" with Principal Component Analysis  

Similar to the knn and regression tree model tries, I wanted to see whether PCA could reduce the number of predictors needed or help improve the results of models. 

```{r PCArcmdr}
#PCA with Recommenderlab
pca <- prcomp(tinyTrain[,c(11:30)], center = FALSE, scale. = FALSE)
axes <- predict(pca, newdata = tinyTrain)
pcaData <- cbind(tinyTrain, axes)
pcaSet <- pcaData[,c(1:10,31:45)]

#Convert the dataframe to a rating matrix. 
pcaSet <- as(pcaSet, "realRatingMatrix")

#Create an evaluation scheme. 
e <- evaluationScheme(pcaSet, method = "cross-validation", train = 0.5, k = 3, given = -1)
e

#Evaluate all methods.
algos <- list(
  UBCF = list(name = "UBCF", param = NULL),
  IBCF = list(name = "IBCF", param = NULL),
  SVD = list(name = "SVD", param = NULL),
  SVDF = list(name = "SVDF", param = NULL),
  AR = list(name = "AR", param = NULL),
  POPULAR = list(name = "POPULAR", param = NULL),
  RANDOM = list(name = "RANDOM", param = NULL),
  RERECOMMEND = list(name = "RERECOMMEND", param = NULL)
)

PCAevlist <- evaluate(e, algos, type = "ratings")

plot(PCAevlist, legend = "topright")

PCAevResults <- avg(PCAevlist)
PCAevResults
```

As we can see from the result sheet, POPULAR method still had the lowest RMSE at 3.17. I created a model with this method. Since a single method model would most likely have similar or worse results from the non-PCA test, I decided to go straight into creating a hybrid recommender to see whether PCA would improve the strangely high RMSE. 
 
```{r PCAHybrid}
#Use the top 3 models of different kinds to create a hybrid recommender. 
PCAHybrid <- HybridRecommender(
  Recommender(getData(e, "train"), "UBCF"),
  Recommender(getData(e, "train"),"SVDF"),
  Recommender(getData(e, "train"), "POPULAR"),
  weights = NULL
)

PCAHybrid

#Create predictions
PCAPred <- recommenderlab::predict(PCAHybrid, getData(e, "known"), type = "ratings")
PCAPred

#Calculate accuray
PCAAccu <- colMeans(as.data.frame(calcPredictionAccuracy(PCAPred, getData(e, "unknown"), byUser = TRUE)))
PCAAccu
```
As we can see from the results, RMSE was not improved and still very high. 

Finally, I tested the POPULAR model on the original validation set. Before doing so, I needed to remove some objects that's no longer used to release enough RAM for the larger data set (although it was already slimed down earlier).

```{r Removes}
rm(pca)
rm(axes)
rm(axesTest)
rm(pcaData)
rm(pcaSet)
rm(tinyTrain)
rm(tinyTest)
rm(train_knn)
rm(train_rf)
rm(y_hat_rf)
rm(y_hat_knn)
rm(fit_rf)
rm(fit_knn)
```

Then, I did the final test on the validation set. 
```{r validation}
library(recommenderlab)
library(dplyr)
#PCA with Recommenderlab
pcaVali <- prcomp(validation[,c(11:30)], center = FALSE, scale. = FALSE)
axesVali <- predict(pcaVali, newdata = validation)
pcaDataVali <- cbind(validation, axesVali)
pcaSetVali <- pcaDataVali[,c(1:10,31:45)]


#Convert the dataframe to a rating matrix. 
pcaRecomSetVali <- as(pcaSetVali, "realRatingMatrix")

#Create an evaluation scheme. 
eRecomSetVali <- evaluationScheme(pcaRecomSetVali, method = "cross-validation", train = 0.5, k = 3, given = -1)
eRecomSetVali


#Create the model
rRecomSetVali <- HybridRecommender(
  Recommender(getData(eRecomSetVali, "train"), "UBCF"),
  Recommender(getData(eRecomSetVali, "train"),"SVDF"),
  Recommender(getData(eRecomSetVali, "train"), "POPULAR"),
  weights = NULL
)
rRecomSetVali

#Create predictions
pRecomSetVali <- recommenderlab::predict(rRecomSetVali, getData(eRecomSetVali, "known"), type = "ratings")
pRecomSetVali

#Calculate accuray
AccuVali <- colMeans(as.data.frame(calcPredictionAccuracy(pRecomSetVali, getData(eRecomSetVali, "unknown"), byUser = TRUE)))
AccuVali
```
As we can see, unfortunately, the model didn't work well on the validation set. 

One thing I would like to note was that through my experience with the "recommenderlab" package, my RMSE outcome changed drastically. The best-result range was from 1.2 to 3.6. I would highly suspect that it was due to the structure of my data set, as I had to modify the training sets due to function errors from the package. It would be interesting to dig further into this finding in the future. 

VII. Results 
The results of this project were bad from the view of RMSE perspective but good from the learning perspective. As for RMSE, I really couldn't find a better method to improve the outcome within the time frame that I was able to work. I would love to see other people's method and improve my logic of problem solving, especially because I saw that several people in the discussion panel were able to crack the code and create a model with great RMSE outcomes. On the bright side, I was able to transfer the data sets into the form that I wanted with all the wrangling functions and apply various methods of machine learning. I did spend a lot of time on this and learned a lot, so I would think that this was a great experience. Thank you for reviewing my report and please provide constructive feedback!
