---
title: "MovieLens Report"
author: "Xiaoshuo Liu"
date: "April 8, 2019"
output:
  word_document: default
  pdf_document: default
---

I. Introduction

The MovieLens data set was provided by the course and was examined for a functional recommendation system. The goal of this project is to construct a recommendation model that provide the right (or closest) movies to a user based on the potential that a high rating will be given by the person. 

After testing through different models, I found that although k nearest neighbors and random forest methods would be used to create a model, their RMSEs were too high to be considered valuable (RMSE-knn~4.92, RMSE-rborist~1.5). Then, I used "recommenderlab" package to run recommendation-specific methods to construct models. The finding was that the "POPULAR" method generated the lowest RMSE at 1.265824. I also used principal component analysis to seek possibility of shrinking down the number of predictors or potentially improving the RMSE. I was able to reduce the number by five while keeping RMSE the same. I spent over a month working part time on this project and decided not to go further due to my time constraints. I thought it would be better to submit the project than to give up after numerous hours of work. To my fellow graders, I would love to hear your feedback so I could possibly improve the model further in the future when I have the time to work on it again.    

I would like to note that I use train sets to train the model, test sets to validate, and validation sets to do the final test. This confusion was created due to my initial impression with the "edx" and "validation" sets. I thought that "edx" would be used for train and test while "validation" was for the final validation. The validation set was only used once to test the final model. 

Another thing that I wanted to point out was that because I had to do this project on a laptop, the processing power was rather limited. The laptop was my only PC device to work on, and I had to use it for other things. Therefore, I had to shrink down the size of the sets so I could get results out for a reasonable amount of time. This could be a reason why my final model was not even good enough to reach the highest RMSE in the rubric. 

II. Preparing the Data

The main data sets were generated by the script provided on the course page. Set edx was for training and testing while Set validation was for validating the model. The course has already required us to explore the data set, so I skipped the exploratory analysis. Knowing that the date and time in the set were labeled in the timestamp format, I first transformed the timestamp format to normal date and time format, then splitted them into separate colunms for easier analysis later. 

First, I loaded the original data. 
```{r original-data}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
The data sets were really large, so I had to slash the size into a quarter. 
```{r quarter}
edx <- sample_n(edx, 2250000, replace = FALSE)
validation <- sample_n(validation, 250000, replace = FALSE)
```


Then, I separated the dates and time. 

```{r date-time}
#Load all the packages needed. 
library(tidyverse)
library(randomForest)
library(rpart)
library(caret)
library(purrr)
library(chron)
library(stringr)


#Transform timestamp into normal date/time format
edx <- edx %>% mutate(timestamp = as.POSIXct(edx$timestamp, origin = "1970-01-01", tz = "GMT"))

validation <- validation %>% mutate(timestamp = as.POSIXct(validation$timestamp, origin = "1970-01-01", tz = "GMT"))


#Splitting date and time
edx <- edx %>% separate(col = timestamp, into = c("date", "time"), sep = " ")

validation <- validation %>% separate(col = timestamp, into = c("date", "time"), sep = " ")

#Check the format of the two new columns
class(edx$date)
class(edx$date)

#Separate year, month, and date from the "date" column
datePrep <- edx$date
ratingYear <- str_sub(string = datePrep, start = 1, end = 4)
ratingMonth <- str_sub(string = datePrep, start = 6, end = 7)
ratingDay <- str_sub(string = datePrep, start = 9, end = 10)
edx <- cbind(edx, ratingYear)
edx <- cbind(edx, ratingMonth)
edx <- cbind(edx, ratingDay)

datePrep <- validation$date
ratingYear <- str_sub(string = datePrep, start = 1, end = 4)
ratingMonth <- str_sub(string = datePrep, start = 6, end = 7)
ratingDay <- str_sub(string = datePrep, start = 9, end = 10)
validation <- cbind(validation, ratingYear)
validation <- cbind(validation, ratingMonth)
validation <- cbind(validation, ratingDay)

#Remove the original "date" column to slim down the set. 
edx <- edx[, -4]
validation <- validation[, -4]

#Separate hour, minute, and second from the "time" column.
timePrep <- edx$time
ratingHour <- str_sub(string = timePrep, start = 1, end = 2)
ratingMin <- str_sub(string = timePrep, start = 4, end = 5)
ratingSec <- str_sub(string = timePrep, start = 7, end = 8)
edx <- cbind(edx, ratingHour)
edx <- cbind(edx, ratingMin)
edx <- cbind(edx, ratingSec)

timePrep <- validation$time
ratingHour <- str_sub(string = timePrep, start = 1, end = 2)
ratingMin <- str_sub(string = timePrep, start = 4, end = 5)
ratingSec <- str_sub(string = timePrep, start = 7, end = 8)
validation <- cbind(validation, ratingHour)
validation <- cbind(validation, ratingMin)
validation <- cbind(validation, ratingSec)

#Remove the original "time" column to slim down the set. 
edx <- edx[, -4]
validation <- validation[, -4]

#Remove all the values created during this process. 
rm(datePrep)
rm(ratingDay)
rm(ratingHour)
rm(ratingMin)
rm(ratingMonth)
rm(ratingSec)
rm(ratingYear)
rm(timePrep)
```

After getting the rating date and time prepared. I also extracted the release year out of the "title" column as a variable. 

```{r releaseYear}
#Create a column of release year. 
releasePrep <- edx$title
releaseYear <- str_sub(string = releasePrep, start = -5, end = -2)
edx <- cbind(edx, releaseYear)

releasePrep <- validation$title
releaseYear <- str_sub(string = releasePrep, start = -5, end = -2)
validation <- cbind(validation, releaseYear)

#Remove the values no longer used. 
rm(releasePrep)
rm(releaseYear)
```

I also converted the "genres" column into a series of columns that are dummy variables for later analysis. 

```{r edx genreSplit}
library(dplyr)
library(tidyr)

#Separate genres under the same title 
edx <- separate_rows(data = edx, genres, sep = "\\|", convert = FALSE)

#Transform character values into dummy variables
edx <- edx %>% mutate(Drama = ifelse(genres == "Drama", 1, 0)) %>%
  mutate(Crime = ifelse(genres == "Crime", 1, 0)) %>%
  mutate(Action = ifelse(genres == "Action", 1, 0)) %>%
  mutate(Adventure = ifelse(genres == "Adventure", 1, 0)) %>%
  mutate(Sci_Fi = ifelse(genres == "Sci-Fi", 1, 0)) %>%
  mutate(Thriller = ifelse(genres == "Thriller", 1, 0)) %>%
  mutate(Comedy = ifelse(genres == "Comedy", 1, 0)) %>%
  mutate(Mystery = ifelse(genres == "Mystery", 1, 0)) %>%
  mutate(Romance = ifelse(genres == "Romance", 1, 0)) %>%
  mutate(Animation = ifelse(genres == "Animation", 1, 0)) %>%
  mutate(Children = ifelse(genres == "Children", 1, 0)) %>%
  mutate(Fantasy = ifelse(genres == "Fantasy", 1, 0)) %>%
  mutate(War = ifelse(genres == "War", 1, 0)) %>%
  mutate(Horror = ifelse(genres == "Horror", 1, 0)) %>%
  mutate(Musical = ifelse(genres == "Musical", 1, 0)) %>%
  mutate(Western = ifelse(genres == "Western", 1, 0)) %>%
  mutate(Film_Noir = ifelse(genres == "Film-Noir", 1, 0)) %>%
  mutate(Documentary = ifelse(genres == "Documentary", 1, 0)) %>%
  mutate(IMAX = ifelse(genres == "IMAX", 1, 0)) %>%
  mutate(no_genres_listed = ifelse(genres == "(no genres listed)", 1, 0))

#Deleting the "genres" column after converting all genres into dummy variables and title column (assume movieId would just do the same). 
edx <- within(edx, rm(genres))
edx <- within(edx, rm(title))

```

After all the foundation work, partitions were created for modeling later. 
```{r partitions}
#Creating Partitions
rating <- edx$rating
set.seed(1)
testIndex <- createDataPartition(rating, times = 1, p = 0.5, list = FALSE)
trainSet <- edx[-testIndex,]
testSet <- edx[testIndex,]

#Making sure that the two sets share users and movies. 
testSet <- testSet %>% 
          semi_join(trainSet, by = "movieId") %>% 
          semi_join(trainSet, by = "userId")

#releasing memory
rm(edx)
rm(testIndex)
rm(rating)
```

Splitting genres for the validation set. It was done after removing edx because of RAM limit. 
```{r validation genres split}
#Applying the same method to the validation set
validation <- separate_rows(data = validation, genres, sep = "\\|", convert = FALSE)

validation <- validation %>% mutate(Drama = ifelse(genres == "Drama", 1, 0)) %>%
  mutate(Crime = ifelse(genres == "Crime", 1, 0)) %>%
  mutate(Action = ifelse(genres == "Action", 1, 0)) %>%
  mutate(Adventure = ifelse(genres == "Adventure", 1, 0)) %>%
  mutate(Sci_Fi = ifelse(genres == "Sci-Fi", 1, 0)) %>%
  mutate(Thriller = ifelse(genres == "Thriller", 1, 0)) %>%
  mutate(Comedy = ifelse(genres == "Comedy", 1, 0)) %>%
  mutate(Mystery = ifelse(genres == "Mystery", 1, 0)) %>%
  mutate(Romance = ifelse(genres == "Romance", 1, 0)) %>%
  mutate(Animation = ifelse(genres == "Animation", 1, 0)) %>%
  mutate(Children = ifelse(genres == "Children", 1, 0)) %>%
  mutate(Fantasy = ifelse(genres == "Fantasy", 1, 0)) %>%
  mutate(War = ifelse(genres == "War", 1, 0)) %>%
  mutate(Horror = ifelse(genres == "Horror", 1, 0)) %>%
  mutate(Musical = ifelse(genres == "Musical", 1, 0)) %>%
  mutate(Western = ifelse(genres == "Western", 1, 0)) %>%
  mutate(Film_Noir = ifelse(genres == "Film-Noir", 1, 0)) %>%
  mutate(Documentary = ifelse(genres == "Documentary", 1, 0)) %>%
  mutate(IMAX = ifelse(genres == "IMAX", 1, 0)) %>%
  mutate(no_genres_listed = ifelse(genres == "(no genres listed)", 1, 0))


validation <- within(validation, rm(genres))
validation <- within(validation, rm(title))
```


III. Predictive Modeling

Although in the lectures, Dr. Irizarry used matrix factorization to construct his sample model, I want to try the two predictive models, K nearest neighbors and regression tree. I noticed that the train and test sets were still quite big for testing models, so I created a smaller set. The "movieID" column was also removed because it caused RAM error on my computer which made it unable to complete model training and testing. 

```{r Tiny}
#Create smaller dataset for model testing
set.seed(1)
tinyTrain <- sample_n(trainSet, 100000, replace = FALSE)
tinyTest <- sample_n(testSet, 100000, replace = FALSE)
#Eliminating "movieID" 
tinyTrain <- within(tinyTrain, rm(movieId))
tinyTest <- within(tinyTest, rm(movieId))
```

1. K Nearest Neighbors
To reduce lengthy processing time, I adopted the control from the lectures. 
```{r knn}
#Create dataframe for predictors. 
predictors <- within(tinyTrain, rm(rating))
#k nearest neighbors
control <- trainControl(method = "cv", number = 5, p = .8)
pred <- tinyTrain[, 2]
train_knn <- train(predictors, pred,
                   method = "knn",
                   tuneGrid = data.frame(k = c(1,3,5,7,9,11,13,15,17,19,21,23,25)),
                   trControl = control)
train_knn$bestTune
pred <- as.factor(pred)
fit_knn <- knn3(predictors, pred, k = train_knn$bestTune)
testPredictors <- within(tinyTest, rm(rating))
y_hat_knn <- predict(fit_knn,
                     testPredictors,
                     type = "class")
testPred <- tinyTest[,2]
cm_knn <- confusionMatrix(as.factor(y_hat_knn), as.factor(testPred))
rmse_knn <- RMSE(as.numeric(testPred), as.numeric(y_hat_knn))
cm_knn
rmse_knn
```
The result showed an accuracy of ~25% and a terrible RMSE of 4.92. Clearly, knn was not able to construct a high quality recommendation system. 


2. Classification Tree (Rborist w/ five-fold cross validation)
To reduce lengthy processing time, I adopted the control from the lectures. 
```{r rborist}
library(Rborist)
control <- trainControl(method = "cv", number = 5, p = 0.8)
grid <- expand.grid(minNode = c(2,5), predFixed = c(5,15,20))
pred <- tinyTrain[, 2]

train_rf <- train(predictors,
                  pred,
                  method = "Rborist",
                  nTree = 50,
                  trControl = control,
                  tuneGrid = grid, 
                  nSamp = 5000)
ggplot(train_rf)
train_rf$bestTune

fit_rf <- Rborist(predictors, pred, 
                  nTree = 1000,
                  minNode = train_rf$bestTune$minNode,
                  predFixed = train_rf$bestTune$predFixed)
testPredictors <- within(tinyTest, rm(rating))
testPred <- tinyTest[,2]
pred_rf <- predict(fit_rf, testPredictors)
y_hat_rf <- as.factor(levels(pred)[predict(fit_rf, testPredictors)$yPred])
testPred <- as.factor(testPred)
cm_rf <- confusionMatrix(as.factor(y_hat_rf), as.factor(testPred))
rmse_rf <- RMSE(as.numeric(testPred), as.numeric(y_hat_rf))
cm_rf
rmse_rf
```
The result showed an accuracy of ~25%, which was similar to the knn result. However, the RMSE improved from 4.92 to ~1.52. Based on the project's rubric, the RMSE was still not good enough. It wasn't worth testing on the validation set. 

In conclusion, both knn and regression tree methods failed to construct satisfying recommendation systems. Before moving into matrix factorization, I wanted to try principal component analysis and see if the procedure could help improve the results of knn and regression tree a bit. 

IV. Principal Component Analysis

There are over twenty columns which represent different genres in the set. This has caused some serious lags on processing time for modeling. I wanted to find out whether a PCA can shrink down the number of variables needed for the prediction. 

```{r PCA}
#Using Principal Component Analysis to shrink down the number of predictors

pca <- prcomp(tinyTrain[,c(11:30)], center = FALSE, scale. = FALSE)
pcaTest <- prcomp(tinyTest[,c(11:30)], center = FALSE, scale. = FALSE)

summary(pca)

#Here we can see that the first 15 components capture ~98% of the variability. Let's use these 15 which represent genres and the rest of regular predictors. 

axes <- predict(pca, newdata = tinyTrain)
axesTest <- predict(pcaTest, newdata = tinyTest)

pcaData <- cbind(tinyTrain, axes)
pcaTestData <- cbind(tinyTest, axesTest)
```

After getting the PCA variables, I tested them with the k nearest neighbors model. 
```{r PCAknn}
#Eliminating MovieID
pcaData <- within(pcaData, rm(movieId))
pcaTestData <- within(pcaTestData, rm(movieId))

pcaData <- within(pcaData, rm(date))
pcaTestData <- within(pcaTestData, rm(date))

#Create dataframe for predictors and predicted values. 
predictors <- pcaData[,c(4:10,31:45)]
pred <- pcaData %>% select(rating)

#k nearest neighbors
control <- trainControl(method = "cv", number = 2, p = .9)
pred <- pcaData[, 2]
train_knn <- train(predictors, pred,
                   method = "knn",
                   tuneGrid = data.frame(k = c(1,3,5,7,9,11,13)),
                   trControl = control)
train_knn$bestTune
pred <- as.factor(pred)
fit_knn <- knn3(predictors, pred, k = 13)
testPredictors <- pcaTestData[,c(4:10,31:45)]
y_hat_knn <- predict(fit_knn,
                     testPredictors,
                     type = "class")
testPred <- pcaTestData[,2]
cm <- confusionMatrix(y_hat_knn, factor(testPred))
cm$overall["Accuracy"]
RMSE(c(y_hat_knn), c(testPred))
```
As the result entailed, the PCA variables didn't improve either the accuracy or the RMSE of the model. I decided not to test it on the random forest model due to this lack of improvement. It was about time to use the matrix factorization method taught in the lectures. 

V. "Recommenderlab" Package

Dr. Irizarry mentioned the "recommenderlab" package at the end of his lecture on a recommendation system. After some research, I learned that the package was specifically written for recommendation system research. The methods available are very similar to the ones introduced by Dr. Irizarry in his lectures. Therefore, instead of replicating and improving the process used in the lecture, I decided to utilize the package for further model construction. For more detailed description of functions that were used, please view the package's document. 

The Recommenderlab package had a function called "evaluate", which evaluates the outcome of one or more methods with the same data set. The methods were user-based collaborative filtering, item-based collaborative filtering, singular value decomposition(SVD) with column-mean imputation, funk SVD, association rule-based recommender, popular items, randomly chosen items for comparison, and re-recommend liked items. Therefore, I decided to evaluate all the methods first before putting them into training. 

```{r rmdr}
#Using the package "recommendarlab" 
library(recommenderlab)
library(dplyr)

#Getting a smaller training set for time efficiency. 
set.seed(1)
recomTrain <- sample_n(tinyTrain, 30000, replace = FALSE)

#Convert the dataframe to a rating matrix. 
recomTrain <- as(recomTrain, "realRatingMatrix")

#Create an evaluation scheme. 
e <- evaluationScheme(recomTrain, method = "cross-validation", k = 5, given = -1)
e

#Evaluate different methods 
algos <- list(
  UBCF = list(name = "UBCF", param = NULL),
  IBCF = list(name = "IBCF", param = NULL),
  SVD = list(name = "SVD", param = NULL),
  SVDF = list(name = "SVDF", param = NULL),
  AR = list(name = "AR", param = NULL),
  POPULAR = list(name = "POPULAR", param = NULL),
  RANDOM = list(name = "RANDOM", param = NULL),
  RERECOMMEND = list(name = "RERECOMMEND", param = NULL)
)

evlist <- evaluate(e, algos, type = "ratings")

plot(evlist, legend = "topright")
avg(evlist)
```

As we can see from the graph and the result sheet, the method "POPULAR" (based on popular items across users) had the lowest RMSE. I created a model with this method.
```{r POPULAR}
#Create POPULAR recommender
r <- Recommender(getData(e, "train"), "POPULAR")
r

#Create predictions
p <- recommenderlab::predict(r, getData(e, "known"), type = "ratings")
p

#Calculate accuray
Accu <- colMeans(as.data.frame(calcPredictionAccuracy(p, getData(e, "unknown"), byUser = TRUE)))
```
As we can see from the result, the RMSE corresponded to the result from the evaluation process. It was still far from the desired RMSE range provided by the rubric. Therefore, I wanted to ensemble several methods to improve the results. 

The "recommenderlab" package offered a "Hybrid Recommender" function, which essnetially allowed combining several methods for one recommendation system. I chose the top three results, user-based collaborative filtering (UBCF), item-based collaborative filtering (IBCF), and popular items (POPULAR) from the evaluation results and see if the RMSE would be improved. 

```{r Hybrid}
#Create hybrid recommender
rHy <- HybridRecommender(
  Recommender(getData(e, "train"), "UBCF"),
  Recommender(getData(e, "train"),"IBCF"),
  Recommender(getData(e, "train"),"POPULAR"),
  weights = NULL
)
rHy

#Create predictions
pHy <- recommenderlab::predict(rHy, getData(e, "known"), type = "ratings")
pHy

#Calculate accuray
AccuHy <- colMeans(as.data.frame(calcPredictionAccuracy(pHy, getData(e, "unknown"), byUser = TRUE)))
```
Stangely enough, the hybrid model's RMSE was significantly higher than all the single models' results. Therefore, the POPULAR model was still the best performer. 

VI. "Recommenderlab" with Principal Component Analysis  

Similar to the knn and regression tree model tries, I wanted to see whether PCA could reduce the number of predictors needed or help improve the results of models. 

```{r PCArcmdr}
#PCA with Recommenderlab
pca <- prcomp(tinyTrain[,c(11:30)], center = FALSE, scale. = FALSE)
axes <- predict(pca, newdata = tinyTrain)
pcaData <- cbind(tinyTrain, axes)
pcaSet <- pcaData[,c(1:10,31:45)]

#Using the package "recommendarlab" 
library(recommenderlab)
library(dplyr)

#Getting a smaller training set for time efficiency. 
set.seed(1)
pcaSet <- sample_n(pcaSet, 30000, replace = FALSE)

#Convert the dataframe to a rating matrix. 
pcaSet <- as(pcaSet, "realRatingMatrix")

#Create an evaluation scheme. 
e <- evaluationScheme(pcaSet, method = "cross-validation", train = 0.8, k = 5, given = -1)
e

#Evaluate all methods.
algos <- list(
  UBCF = list(name = "UBCF", param = NULL),
  IBCF = list(name = "IBCF", param = NULL),
  SVD = list(name = "SVD", param = NULL),
  SVDF = list(name = "SVDF", param = NULL),
  AR = list(name = "AR", param = NULL),
  POPULAR = list(name = "POPULAR", param = NULL),
  RANDOM = list(name = "RANDOM", param = NULL),
  RERECOMMEND = list(name = "RERECOMMEND", param = NULL)
)

evlist <- evaluate(e, algos, type = "ratings")

plot(evlist, legend = "topright")

evResults <- avg(evlist)
```

As we can see from the result sheet, POPULAR method still had the lowest and same RMSE at 1.26. I created a model with this method. Since a single method model would most likely match the results from the non-PCA test, I decided to go straight into creating a hybrid recommender to see whether PCA would improve the strangely high RMSE. 
 
```{r PCAHybrid}
#Use the top 3 models to create a hybrid recommender. 
PCAHybrid <- HybridRecommender(
  Recommender(getData(e, "train"), "UBCF"),
  Recommender(getData(e, "train"),"IBCF"),
  Recommender(getData(e, "train"),"POPULAR"),
  weights = NULL
)

PCAHybrid

#Create predictions
PCAPred <- recommenderlab::predict(PCAHybrid, getData(e, "known"), type = "ratings")
PCAPred

#Calculate accuray
PCAAccu <- colMeans(as.data.frame(calcPredictionAccuracy(PCAPred, getData(e, "unknown"), byUser = TRUE)))
```
As we can see from the results, the RMSE was still very high that the POPULAR model was still the best performer. 

Finally, I tested the POPULAR model on the original validation set. 
```{r validation}
#Test the POPULAR model on the original validation set. 
##Convert the data frame into a rating matrix. 
recomVali <- as(validation, "realRatingMatrix")

#Create predictions
pVali <- recommenderlab::predict(r, recomVali, type = "ratings")
pVali

#Calculate accuray
AccuHy <- colMeans(as.data.frame(calcPredictionAccuracy(pVali, validation[,3], byUser = TRUE)))
```


VII. Results 
